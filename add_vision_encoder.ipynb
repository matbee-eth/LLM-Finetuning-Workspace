{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matbee-eth/LLM-Finetuning-Workspace/blob/main/add_vision_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8d42808ee0b9caf1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "!pip install torch transformers accelerate vllm xformers pillow huggingface_hub"
      ],
      "id": "8d42808ee0b9caf1"
    },
    {
      "metadata": {
        "id": "initial_id"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, Mistral3ForConditionalGeneration, AutoTokenizer, AutoProcessor"
      ],
      "id": "initial_id"
    },
    {
      "metadata": {
        "id": "1bf023260f92f1d5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "mistral_small_path = \"unsloth/Mistral-Small-3.1-24B-Instruct-2503\"\n",
        "magistral_path = \"mistralai/Magistral-Small-2506\"\n",
        "\n",
        "magistral = AutoModelForCausalLM.from_pretrained(\n",
        "     magistral_path,\n",
        "     torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "mistral_small = Mistral3ForConditionalGeneration.from_pretrained(\n",
        "     mistral_small_path,\n",
        "     torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "magistral_vision = Mistral3ForConditionalGeneration.from_pretrained(\n",
        "     magistral_path,\n",
        "     torch_dtype=torch.bfloat16,\n",
        "     ignore_mismatched_sizes=True\n",
        ")"
      ],
      "id": "1bf023260f92f1d5"
    },
    {
      "metadata": {
        "id": "eda433880c8eefac"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "state_dict_magistral = magistral.state_dict()\n",
        "state_dict_small = mistral_small.state_dict()\n",
        "\n",
        "print(\"------- Magistral state dict -------\")\n",
        "for name, params in state_dict_magistral.items():\n",
        "    print(f\"{name} | Shape: {params.shape}\")\n",
        "print(\"------- Magistral state dict -------\")\n",
        "\n",
        "print(\"------- Small state dict -------\")\n",
        "for name, params in state_dict_small.items():\n",
        "    print(f\"{name} | Shape: {params.shape}\")\n",
        "print(\"------- Small state dict -------\")"
      ],
      "id": "eda433880c8eefac"
    },
    {
      "metadata": {
        "id": "d35588ad79f948a8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "new_state_dict = {}\n",
        "\n",
        "for k, v in state_dict_magistral.items():\n",
        "    new_key = k\n",
        "    if \"lm_head\" not in k:\n",
        "        new_key = k.replace(\"model.\", \"\")\n",
        "        new_key = (\"model.language_model.\" + new_key).strip()\n",
        "    new_state_dict[new_key] = v\n",
        "    print(f\"Added language layer: {new_key}\")\n",
        "\n",
        "for k, v in state_dict_small.items():\n",
        "     if \"vision_tower\" in k or \"multi_modal_projector\" in k:\n",
        "          new_state_dict[k] = state_dict_small[k]\n",
        "          print(f\"Added vision layer: {k}\")"
      ],
      "id": "d35588ad79f948a8"
    },
    {
      "metadata": {
        "id": "99ad18d8e94428ef"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "load_result = magistral_vision.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "print(\"\\n------- Load Result -------\")\n",
        "print(f\"Missing keys: {load_result.missing_keys}\")\n",
        "print(f\"Unexpected keys: {load_result.unexpected_keys}\")"
      ],
      "id": "99ad18d8e94428ef"
    },
    {
      "metadata": {
        "id": "4ae0a0f60bdb4954"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "output_path = \"/model_weights/magistral_vision\"\n",
        "\n",
        "magistral_vision.save_pretrained(output_path)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(mistral_small_path)\n",
        "processor.save_pretrained(output_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(mistral_small_path)\n",
        "tokenizer.save_pretrained(output_path)"
      ],
      "id": "4ae0a0f60bdb4954"
    },
    {
      "metadata": {
        "id": "279444bc55df8d4a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from vllm import LLM\n",
        "from vllm.sampling_params import SamplingParams\n",
        "\n",
        "llm = LLM(\n",
        "     model=output_path,\n",
        "     max_model_len=8192,\n",
        ")"
      ],
      "id": "279444bc55df8d4a"
    },
    {
      "metadata": {
        "id": "cbc0812f6215812"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "sampling_params = SamplingParams(\n",
        "    max_tokens=4096,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "prompt = \"Describe this image in one sentence.\"\n",
        "image_url = \"https://picsum.photos/id/237/200/300\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"A user will ask you to solve a task. You should first draft your thinking process (inner monologue) until you have derived the final answer. Afterwards, write a self-contained summary of your thoughts (i.e. your summary should be succinct but contain all the critical steps you needed to reach the conclusion). You should use Markdown to format your response. Write both your thoughts and summary in the same language as the task posed by the user. NEVER use \\\\boxed{} in your response.\n",
        "\n",
        "        Your thinking process must follow the template below:\n",
        "        <think>\n",
        "        Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate a correct answer.\n",
        "        </think>\n",
        "\n",
        "        Here, provide a concise summary that reflects your reasoning and presents a clear final answer to the user. Don't mention that this is a summary.\n",
        "\n",
        "        Problem:\n",
        "\n",
        "        \"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "\n",
        "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
        "print(\"-------\")\n",
        "print(outputs[0].outputs[0].text)\n",
        "print(\"-------\")"
      ],
      "id": "cbc0812f6215812"
    },
    {
      "metadata": {
        "id": "b97fedef16e0ca9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "repo_id = \"OptimusePrime/Magistral-Small-2506-Vision\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "api.upload_large_folder(\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    folder_path=output_path,\n",
        ")"
      ],
      "id": "b97fedef16e0ca9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}